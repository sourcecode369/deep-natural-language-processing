{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate Music with Transofrmers.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNYAxCGSJfTWz5gRhi/fOoF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourcecode369/deep-nlp/blob/master/music%20generation/Generate_Music_with_Transofrmers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQNiYD3L7jbM",
        "colab_type": "code",
        "outputId": "ea607138-52d4-4775-e560-2f52015101ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# clone the repository for midi songs and notes file\n",
        "!git clone https://github.com/Skuldur/Classical-Piano-Composer.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Classical-Piano-Composer'...\n",
            "remote: Enumerating objects: 334, done.\u001b[K\n",
            "remote: Total 334 (delta 0), reused 0 (delta 0), pack-reused 334\n",
            "Receiving objects: 100% (334/334), 721.79 MiB | 11.95 MiB/s, done.\n",
            "Resolving deltas: 100% (41/41), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSA4ibwonCj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# move the files outside of the git folder to the main folder\n",
        "!mv Classical-Piano-Composer/midi_songs midi_songs\n",
        "!mv Classical-Piano-Composer/data data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDkqr68pnRF-",
        "colab_type": "code",
        "outputId": "f2270e8c-13d1-4162-e850-b962f5f00936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# check the existence of the files\n",
        "!ls -GFlash --color"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 28K\n",
            "4.0K drwxr-xr-x 1 root 4.0K Jan 12 14:45 \u001b[0m\u001b[01;34m.\u001b[0m/\n",
            "4.0K drwxr-xr-x 1 root 4.0K Jan 12 14:41 \u001b[01;34m..\u001b[0m/\n",
            "4.0K drwxr-xr-x 4 root 4.0K Jan 12 14:45 \u001b[01;34mClassical-Piano-Composer\u001b[0m/\n",
            "4.0K drwxr-xr-x 1 root 4.0K Jan  8 16:41 \u001b[01;34m.config\u001b[0m/\n",
            "4.0K drwxr-xr-x 2 root 4.0K Jan 12 14:44 \u001b[01;34mdata\u001b[0m/\n",
            "4.0K drwxr-xr-x 2 root 4.0K Jan 12 14:44 \u001b[01;34mmidi_songs\u001b[0m/\n",
            "4.0K drwxr-xr-x 1 root 4.0K Dec 18 16:52 \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTgyMsyiwffX",
        "colab_type": "code",
        "outputId": "b9061981-82d5-4ec0-a1db-fbc2ad9e0101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# remove some files in order to run it on colab and prevent it from using all of the ram\n",
        "# the more the data the better the generated sound will be\n",
        "import os\n",
        "files = os.listdir('midi_songs/')\n",
        "file_path = [os.path.join('midi_songs/',i) for i in files]\n",
        "print(f\"Last 10 file paths: {file_path[-10:]}\")\n",
        "print(f\"Number of files: {len(file_path)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last 10 file paths: ['midi_songs/balamb.mid', 'midi_songs/AT.mid', 'midi_songs/cosmo.mid', 'midi_songs/ff8-lfp.mid', 'midi_songs/tpirtsd-piano.mid', 'midi_songs/costadsol.mid', 'midi_songs/Final_Fantasy_Matouyas_Cave_Piano.mid', 'midi_songs/Kingdom_Hearts_Dearly_Beloved.mid', 'midi_songs/FF3_Battle_(Piano).mid', 'midi_songs/FFIX_Piano.mid']\n",
            "Number of files: 92\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyG2EGlrwqQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try different values of retaining the amount data\n",
        "# remove the files\n",
        "for f in file_path[10:]:\n",
        "  os.remove(f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-sHpDFN7zrt",
        "colab_type": "code",
        "outputId": "3879ee47-7f01-473b-d930-efebdd1db819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "# importing necessary modules\n",
        "from __future__ import absolute_import, print_function, unicode_literals, division\n",
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "from music21 import converter, instrument, note, chord \n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization as BatchNorm, LSTM, Activation \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "from keras.utils import plot_model, np_utils"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af3uifhooR08",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_network():\n",
        "  notes = get_notes()\n",
        "  n_vocab = len(set(notes))\n",
        "  network_input, network_output = prepare_sequences(notes, n_vocab)\n",
        "  model = create_network(network_input, n_vocab)\n",
        "  train(model, network_input, network_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfvvXe7Ys5Ah",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_notes():\n",
        "  notes = []\n",
        "  for file in glob.glob(\"midi_songs/*.mid\"):\n",
        "    midi = converter.parse(file)\n",
        "    print(f\"Parsing {file}.\")\n",
        "    notes_to_parse = None\n",
        "    try:\n",
        "      s2 = instrument.partitionByInstrument(midi)\n",
        "      notes_to_parse = s2.parts[0].recurse()\n",
        "    except:\n",
        "      notes_to_parse = midi.flat.notes \n",
        "    for element in notes_to_parse:\n",
        "      if isinstance(element, note.Note):\n",
        "        notes.append(str(element.pitch))\n",
        "      elif isinstance(element, chord.Chord):\n",
        "        notes.append('.'.join(str(n) for n in element.normalOrder))\n",
        "  with open('data/notes','wb') as filepath:\n",
        "    pickle.dump(notes, filepath)\n",
        "  return notes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP69dR30ul8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequences(notes, n_vocab):\n",
        "  sequence_length = 1000\n",
        "  pitchnames = sorted(set(item for item in notes))\n",
        "  note_to_int = dict((note, number) for number, note in enumerate(pitchnames)) \n",
        "  network_input = []\n",
        "  network_output = []\n",
        "\n",
        "  for i in range(0, len(notes)-sequence_length, 1):\n",
        "    sequence_in = notes[i:i+sequence_length]\n",
        "    sequence_out = notes[i+sequence_length]\n",
        "    network_input.append([note_to_int[char] for char in sequence_in])\n",
        "    network_output.append(note_to_int[sequence_out])\n",
        "  n_patterns = len(network_input)\n",
        "  network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "  network_input = network_input / float(n_vocab)\n",
        "  network_output = np_utils.to_categorical(network_output)\n",
        "  return (network_input, network_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5pvHOWAwNuq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(\n",
        "      128,\n",
        "      input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "      recurrent_dropout=0.3,\n",
        "      return_sequences=True\n",
        "  ))\n",
        "  model.add(LSTM(64, recurrent_dropout=0.3))\n",
        "  model.add(BatchNorm())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(32))\n",
        "  model.add(Activation('relu'))\n",
        "  model.add(BatchNorm())\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6z4jI-gxR8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, network_input, network_output):\n",
        "  filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(\n",
        "      filepath,\n",
        "      monitor='loss',\n",
        "      verbose=0,\n",
        "      save_best_only=True,\n",
        "      mode=\"min\"\n",
        "  )\n",
        "  reduce_lr = ReduceLROnPlateau(monitor='loss', verbose=1, patience=4, mode='min', min_lr=0.000001, factor=0.3)\n",
        "  early_stopping = EarlyStopping(monitor=\"loss\",verbose=1, patience=6, mode='min')\n",
        "  callbacks_list = [checkpoint, reduce_lr, early_stopping]\n",
        "\n",
        "  model.fit(network_input, network_output, epochs=200, batch_size=512, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwxuUyGOx_ji",
        "colab_type": "code",
        "outputId": "73a63712-d050-484c-afa1-c752a274524b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  train_network()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing midi_songs/ViviinAlexandria.mid.\n",
            "Parsing midi_songs/Still_Alive-1.mid.\n",
            "Parsing midi_songs/roseofmay-piano.mid.\n",
            "Parsing midi_songs/sobf.mid.\n",
            "Parsing midi_songs/Gold_Silver_Rival_Battle.mid.\n",
            "Parsing midi_songs/sera_.mid.\n",
            "Parsing midi_songs/ultimafro.mid.\n",
            "Parsing midi_songs/FF3_Third_Phase_Final_(Piano).mid.\n",
            "Parsing midi_songs/ff4pclov.mid.\n",
            "Parsing midi_songs/rufus.mid.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7102/7102 [==============================] - 49s 7ms/step - loss: 5.4944\n",
            "Epoch 2/200\n",
            "7102/7102 [==============================] - 39s 5ms/step - loss: 5.4295\n",
            "Epoch 3/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.3801\n",
            "Epoch 4/200\n",
            "7102/7102 [==============================] - 39s 5ms/step - loss: 5.3358\n",
            "Epoch 5/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.2865\n",
            "Epoch 6/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.2398\n",
            "Epoch 7/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.1712\n",
            "Epoch 8/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.1168\n",
            "Epoch 9/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.0557\n",
            "Epoch 10/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.0192\n",
            "Epoch 11/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.9513\n",
            "Epoch 12/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.9110\n",
            "Epoch 13/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.8444\n",
            "Epoch 14/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.8072\n",
            "Epoch 15/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.7559\n",
            "Epoch 16/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.6998\n",
            "Epoch 17/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.6638\n",
            "Epoch 18/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.6173\n",
            "Epoch 19/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.5818\n",
            "Epoch 20/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.5444\n",
            "Epoch 21/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.5111\n",
            "Epoch 22/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4863\n",
            "Epoch 23/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4570\n",
            "Epoch 24/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4371\n",
            "Epoch 25/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4132\n",
            "Epoch 26/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4134\n",
            "Epoch 27/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3904\n",
            "Epoch 28/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3818\n",
            "Epoch 29/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3626\n",
            "Epoch 30/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3623\n",
            "Epoch 31/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.3489\n",
            "Epoch 32/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3394\n",
            "Epoch 33/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3337\n",
            "Epoch 34/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3405\n",
            "Epoch 35/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3364\n",
            "Epoch 36/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3327\n",
            "Epoch 37/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3260\n",
            "Epoch 38/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3242\n",
            "Epoch 39/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3158\n",
            "Epoch 40/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3200\n",
            "Epoch 41/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.3086\n",
            "Epoch 42/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3142\n",
            "Epoch 43/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3025\n",
            "Epoch 44/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.3014\n",
            "Epoch 45/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3035\n",
            "Epoch 46/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2939\n",
            "Epoch 47/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2966\n",
            "Epoch 48/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2945\n",
            "Epoch 49/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2828\n",
            "Epoch 50/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2854\n",
            "Epoch 51/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2860\n",
            "Epoch 52/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2802\n",
            "Epoch 53/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2717\n",
            "Epoch 54/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2761\n",
            "Epoch 55/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2722\n",
            "Epoch 56/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2688\n",
            "Epoch 57/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2724\n",
            "Epoch 58/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2657\n",
            "Epoch 59/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2630\n",
            "Epoch 60/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2624\n",
            "Epoch 61/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2533\n",
            "Epoch 62/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2525\n",
            "Epoch 63/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2520\n",
            "Epoch 64/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2484\n",
            "Epoch 65/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2495\n",
            "Epoch 66/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2536\n",
            "Epoch 67/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2420\n",
            "Epoch 68/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2447\n",
            "Epoch 69/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2418\n",
            "Epoch 70/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2392\n",
            "Epoch 71/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2317\n",
            "Epoch 72/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2305\n",
            "Epoch 73/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2323\n",
            "Epoch 74/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2261\n",
            "Epoch 75/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2245\n",
            "Epoch 76/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2270\n",
            "Epoch 77/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2127\n",
            "Epoch 78/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2077\n",
            "Epoch 79/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2153\n",
            "Epoch 80/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2101\n",
            "Epoch 81/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2209\n",
            "Epoch 82/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2156\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "Epoch 83/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.1961\n",
            "Epoch 84/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1911\n",
            "Epoch 85/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1854\n",
            "Epoch 86/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1832\n",
            "Epoch 87/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1725\n",
            "Epoch 88/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.1752\n",
            "Epoch 89/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1672\n",
            "Epoch 90/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1771\n",
            "Epoch 91/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1775\n",
            "Epoch 92/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1750\n",
            "Epoch 93/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1625\n",
            "Epoch 94/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1716\n",
            "Epoch 95/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1722\n",
            "Epoch 96/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1716\n",
            "Epoch 97/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1638\n",
            "\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Epoch 98/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1690\n",
            "Epoch 99/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1651\n",
            "Epoch 00099: early stopping\n",
            "Parsing midi_songs/ViviinAlexandria.mid.\n",
            "Parsing midi_songs/Still_Alive-1.mid.\n",
            "Parsing midi_songs/roseofmay-piano.mid.\n",
            "Parsing midi_songs/sobf.mid.\n",
            "Parsing midi_songs/Gold_Silver_Rival_Battle.mid.\n",
            "Parsing midi_songs/sera_.mid.\n",
            "Parsing midi_songs/ultimafro.mid.\n",
            "Parsing midi_songs/FF3_Third_Phase_Final_(Piano).mid.\n",
            "Parsing midi_songs/ff4pclov.mid.\n",
            "Parsing midi_songs/rufus.mid.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "7102/7102 [==============================] - 49s 7ms/step - loss: 5.4944\n",
            "Epoch 2/200\n",
            "7102/7102 [==============================] - 39s 5ms/step - loss: 5.4295\n",
            "Epoch 3/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.3801\n",
            "Epoch 4/200\n",
            "7102/7102 [==============================] - 39s 5ms/step - loss: 5.3358\n",
            "Epoch 5/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.2865\n",
            "Epoch 6/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.2398\n",
            "Epoch 7/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.1712\n",
            "Epoch 8/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.1168\n",
            "Epoch 9/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.0557\n",
            "Epoch 10/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 5.0192\n",
            "Epoch 11/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.9513\n",
            "Epoch 12/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.9110\n",
            "Epoch 13/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.8444\n",
            "Epoch 14/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.8072\n",
            "Epoch 15/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.7559\n",
            "Epoch 16/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.6998\n",
            "Epoch 17/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.6638\n",
            "Epoch 18/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.6173\n",
            "Epoch 19/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.5818\n",
            "Epoch 20/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.5444\n",
            "Epoch 21/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.5111\n",
            "Epoch 22/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4863\n",
            "Epoch 23/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4570\n",
            "Epoch 24/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4371\n",
            "Epoch 25/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4132\n",
            "Epoch 26/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.4134\n",
            "Epoch 27/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3904\n",
            "Epoch 28/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3818\n",
            "Epoch 29/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3626\n",
            "Epoch 30/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3623\n",
            "Epoch 31/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.3489\n",
            "Epoch 32/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3394\n",
            "Epoch 33/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3337\n",
            "Epoch 34/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3405\n",
            "Epoch 35/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3364\n",
            "Epoch 36/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3327\n",
            "Epoch 37/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3260\n",
            "Epoch 38/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3242\n",
            "Epoch 39/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3158\n",
            "Epoch 40/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3200\n",
            "Epoch 41/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.3086\n",
            "Epoch 42/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3142\n",
            "Epoch 43/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3025\n",
            "Epoch 44/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.3014\n",
            "Epoch 45/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.3035\n",
            "Epoch 46/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2939\n",
            "Epoch 47/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2966\n",
            "Epoch 48/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2945\n",
            "Epoch 49/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2828\n",
            "Epoch 50/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2854\n",
            "Epoch 51/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2860\n",
            "Epoch 52/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2802\n",
            "Epoch 53/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2717\n",
            "Epoch 54/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2761\n",
            "Epoch 55/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2722\n",
            "Epoch 56/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2688\n",
            "Epoch 57/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2724\n",
            "Epoch 58/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2657\n",
            "Epoch 59/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2630\n",
            "Epoch 60/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2624\n",
            "Epoch 61/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2533\n",
            "Epoch 62/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2525\n",
            "Epoch 63/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2520\n",
            "Epoch 64/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2484\n",
            "Epoch 65/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2495\n",
            "Epoch 66/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2536\n",
            "Epoch 67/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2420\n",
            "Epoch 68/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2447\n",
            "Epoch 69/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2418\n",
            "Epoch 70/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2392\n",
            "Epoch 71/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2317\n",
            "Epoch 72/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2305\n",
            "Epoch 73/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2323\n",
            "Epoch 74/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2261\n",
            "Epoch 75/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.2245\n",
            "Epoch 76/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2270\n",
            "Epoch 77/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2127\n",
            "Epoch 78/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2077\n",
            "Epoch 79/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2153\n",
            "Epoch 80/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2101\n",
            "Epoch 81/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2209\n",
            "Epoch 82/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.2156\n",
            "\n",
            "Epoch 00082: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
            "Epoch 83/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.1961\n",
            "Epoch 84/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1911\n",
            "Epoch 85/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1854\n",
            "Epoch 86/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1832\n",
            "Epoch 87/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1725\n",
            "Epoch 88/200\n",
            "7102/7102 [==============================] - 37s 5ms/step - loss: 4.1752\n",
            "Epoch 89/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1672\n",
            "Epoch 90/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1771\n",
            "Epoch 91/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1775\n",
            "Epoch 92/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1750\n",
            "Epoch 93/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1625\n",
            "Epoch 94/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1716\n",
            "Epoch 95/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1722\n",
            "Epoch 96/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1716\n",
            "Epoch 97/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1638\n",
            "\n",
            "Epoch 00097: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
            "Epoch 98/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1690\n",
            "Epoch 99/200\n",
            "7102/7102 [==============================] - 38s 5ms/step - loss: 4.1651\n",
            "Epoch 00099: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96WyiqFhyiuJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# try:\n",
        "#   from google.colab import files\n",
        "#   files.upload(\n",
        "#   )\n",
        "# except Exception as ex:\n",
        "#   print(ex)\n",
        "\n",
        "# importing necessary modules\n",
        "from __future__ import absolute_import, print_function, unicode_literals, division\n",
        "import os\n",
        "import gc\n",
        "gc.enable()\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "import glob\n",
        "import pickle\n",
        "import numpy\n",
        "from music21 import converter, instrument, note, chord, stream  \n",
        "from keras.models import Sequential, Model, load_model\n",
        "from keras.layers import Dense, Dropout, BatchNormalization as BatchNorm, LSTM, Activation \n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
        "from keras.utils import plot_model, np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3lwL7N4KTcA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate():\n",
        "    \"\"\" Generate a piano midi file \"\"\"\n",
        "    #load the notes used to train the model\n",
        "    with open('data/notes', 'rb') as filepath:\n",
        "        notes = pickle.load(filepath)\n",
        "\n",
        "    # Get all pitch names\n",
        "    pitchnames = sorted(set(item for item in notes))\n",
        "    # Get all pitch names\n",
        "    n_vocab = len(set(notes))\n",
        "\n",
        "    network_input, normalized_input = prepare_sequences(notes, pitchnames, n_vocab)\n",
        "    model = create_network(normalized_input, n_vocab)\n",
        "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
        "    create_midi(prediction_output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjR_BxuBKWX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_sequences(notes, pitchnames, n_vocab):\n",
        "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
        "    # map between notes and integers and back\n",
        "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    sequence_length = 1000\n",
        "    network_input = []\n",
        "    output = []\n",
        "    for i in range(0, len(notes) - sequence_length, 1):\n",
        "        sequence_in = notes[i:i + sequence_length]\n",
        "        sequence_out = notes[i + sequence_length]\n",
        "        network_input.append([note_to_int[char] for char in sequence_in])\n",
        "        output.append(note_to_int[sequence_out])\n",
        "\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "    # reshape the input into a format compatible with LSTM layers\n",
        "    normalized_input = numpy.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "    # normalize input\n",
        "    normalized_input = normalized_input / float(n_vocab)\n",
        "\n",
        "    return (network_input, normalized_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjM8fhcTKYvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_network(network_input, n_vocab):\n",
        "    \"\"\" create the structure of the neural network \"\"\"\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(\n",
        "        128,\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "        recurrent_dropout=0.3,\n",
        "        return_sequences=True\n",
        "    ))\n",
        "    model.add(LSTM(64, recurrent_dropout=0.3))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(32))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNorm())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(n_vocab))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop')\n",
        "\n",
        "    # Load the weights to each node\n",
        "    model.load_weights('weights.hdf5')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQgjSbbxJulD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
        "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
        "    # pick a random sequence from the input as a starting point for the prediction\n",
        "    start = numpy.random.randint(0, len(network_input)-1)\n",
        "\n",
        "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "    pattern = network_input[start]\n",
        "    prediction_output = []\n",
        "\n",
        "    # generate 500 notes\n",
        "    for note_index in range(500):\n",
        "        prediction_input = numpy.reshape(pattern, (1, len(pattern), 1))\n",
        "        prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "        prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "        index = numpy.argmax(prediction)\n",
        "        result = int_to_note[index]\n",
        "        prediction_output.append(result)\n",
        "\n",
        "        pattern.append(index)\n",
        "        pattern = pattern[1:len(pattern)]\n",
        "\n",
        "    return prediction_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYU4mDXPKcE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_midi(prediction_output):\n",
        "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
        "        from the notes \"\"\"\n",
        "    offset = 0\n",
        "    output_notes = []\n",
        "\n",
        "    # create note and chord objects based on the values generated by the model\n",
        "    for pattern in prediction_output:\n",
        "        # pattern is a chord\n",
        "        if ('.' in pattern) or pattern.isdigit():\n",
        "            notes_in_chord = pattern.split('.')\n",
        "            notes = []\n",
        "            for current_note in notes_in_chord:\n",
        "                new_note = note.Note(int(current_note))\n",
        "                new_note.storedInstrument = instrument.Piano()\n",
        "                notes.append(new_note)\n",
        "            new_chord = chord.Chord(notes)\n",
        "            new_chord.offset = offset\n",
        "            output_notes.append(new_chord)\n",
        "        # pattern is a note\n",
        "        else:\n",
        "            new_note = note.Note(pattern)\n",
        "            new_note.offset = offset\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            output_notes.append(new_note)\n",
        "\n",
        "        # increase offset each iteration so that notes do not stack\n",
        "        offset += 0.5\n",
        "\n",
        "    midi_stream = stream.Stream(output_notes)\n",
        "\n",
        "    midi_stream.write('midi', fp='test_output.mid')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7pVN9pRKQCv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    generate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjkhsNcEK0XN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}